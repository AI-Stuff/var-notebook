{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value-at-risk calculations\n",
    "\n",
    "The basic idea behind the value-at-risk calculation is that we're going to look at the historical returns of a portfolio of securities and run many simulations to determine the range of returns we can expect from these.  We can then predict, over a given time horizon, what our expected loss is at a given probability, e.g., we might say that there is less than a 10% chance that the portfolio will lose more than $1,000,000.\n",
    "\n",
    "Note that this is a didactic example and consequently makes some simplifying assumptions about the composition of the portfolio (i.e., only long positions in common stocks, so no options, dividends, or short selling) and the behavior of the market (i.e., day-to-day return percentages are normally-distributed and independent).  Do not use this code to guide actual investment decisions!\n",
    "\n",
    "## Basic setup\n",
    "\n",
    "Here we import the `pyspark` module and set up a `SparkSession`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "spark = SparkSession.builder.master(\"spark://sparky:7077\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Loading the data\n",
    "\n",
    "We'll start by loading the data (from the WikiEOD dataset of freely-available stock closing prices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = spark.read.load(\"/data/wikieod.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating historical returns\n",
    "\n",
    "We'll use Spark's windowing functions over data frames to determine the percentage change in each security from the previous close to each day's close.  Basically, we'll add a column to our data frame that represents the percentage change from the previous day's close (that is, `lag(\"close\", 1)` when partitioned by ticker symbol and ordered by date) and the current day's close (that is, `col(\"close\")`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import lag, col, avg, variance\n",
    "\n",
    "ddf = df.select(\"ticker\", \"date\", \"close\").withColumn(\"change\", (col(\"close\") / lag(\"close\", 1).over(Window.partitionBy(\"ticker\").orderBy(df[\"date\"])) - 1.0) * 100)\n",
    "ddf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characterizing expected return distributions\n",
    "\n",
    "With the range of changes now available, we can calculate our expected returns for each security.  Since this is a simple example, we'll assume that returns are normal (in the real world, you'd want to use a distribution with heavier tails or a more sophisticated technique altogether).  We can calculate the parameters for each security's distribution as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sqrt\n",
    "mv = ddf.groupBy(\"ticker\").agg(avg(\"change\").alias(\"mean\"), sqrt(variance(\"change\")).alias(\"stddev\"))\n",
    "mv.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are only about 3,000 ticker symbols in our data set, we can easily collect these in driver memory for use in our simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dist_map = mv.rdd.map(lambda r: (r[0], (r[1], r[2]))).collectAsMap()\n",
    "dist_map[\"RHT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting current security prices\n",
    "\n",
    "We'll now identify the latest price for each security in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import first\n",
    "priceDF = ddf.orderBy(\"date\", ascending=False).groupBy(\"ticker\").agg(first(\"close\").alias(\"price\"), first(\"date\").alias(\"date\"))\n",
    "priceDF.show(10)\n",
    "\n",
    "prices = priceDF.rdd.map(lambda r: (r[0], r[1])).collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a simulation\n",
    "\n",
    "We'll now define our simulation.  This involves three steps: \n",
    "\n",
    "1.  We'll start by generating a random portfolio of securities (a map from ticker symbols to values); then, \n",
    "2.  we'll decide how many simulations to run and generate a random seed for each; and, finally\n",
    "3.  we'll actually run the simulation for a given number of training days, updating the value of our portfolio with random returns sampled from the distribution of historical returns.\n",
    "\n",
    "Generating the random portfolio is pretty straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import randint, seed\n",
    "\n",
    "def random_portfolio(symbols):\n",
    "    result = {}\n",
    "    for s in symbols:\n",
    "        result[s] = prices[s] * (randint(1, 1000) * 11)\n",
    "    return result\n",
    "\n",
    "def portfolio_value(pf):\n",
    "    return sum([v for v in pf.values()])\n",
    "\n",
    "seed(0xdea110c8)\n",
    "\n",
    "portfolio = random_portfolio(ddf.select(\"ticker\").distinct().sample(True, 0.01, 0xdea110c8).rdd.map(lambda r: r[0]).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is generating a collection of random seeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seeds(count):\n",
    "    return [randint(0, 1 << 32 - 1) for i in range(count)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a single step of our simulation (and, subsequently, a whole run of the simulation) next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simstep(pf, params, prng):\n",
    "    def daily_return(sym):\n",
    "        mean, stddev = params[sym]\n",
    "        change = (prng.normalvariate(mean, stddev) + 100) / 100.0\n",
    "        return change\n",
    "    return dict([(s, daily_return(s) * v) for s, v in pf.items()])\n",
    "\n",
    "def simulate(seed, pf, params, days):\n",
    "    from random import Random\n",
    "    prng = Random()\n",
    "    prng.seed(seed)\n",
    "    pf = pf.copy()\n",
    "    \n",
    "    for day in range(days):\n",
    "        pf = simstep(pf, params, prng)\n",
    "    return pf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything we need to actually run the simulation.  For each seed, we'll spawn a Spark job to simulate 5 days of activity on our portfolio and then return the total dollar value of our gain or loss at the end of the period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "seed_rdd = sc.parallelize(seeds(10000))\n",
    "bparams = sc.broadcast(dist_map)\n",
    "bpf = sc.broadcast(portfolio)\n",
    "initial_value = portfolio_value(portfolio)\n",
    "\n",
    "results = seed_rdd.map(lambda s: portfolio_value(simulate(s, bpf.value, bparams.value, 5)) - initial_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "simulated_results = list(zip(results.collect(), seed_rdd.collect()))\n",
    "simulated_values = [v for (v, _) in simulated_results]\n",
    "simulated_values.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_ = plt.hist(simulated_values, bins=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xvals = [float(i) / len(simulated_values) for i in range(len(simulated_values))]\n",
    "_ = plt.plot(xvals, simulated_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the market historically trends up, we have a better than even chance of not losing money in our simulation.  We can see the 5% value at risk over our time horizon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simulated_values[int(len(simulated_values) * 0.05)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing random walks\n",
    "\n",
    "Finally, let's look at some example simulation runs to see how the portfolio value changes over time.  We'll take the runs with the best and worst returns and also the runs at each decile.  To visualize our simulation, we'll need a slightly different `simulate` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulate_with_history(seed, pf, params, days):\n",
    "    from random import Random\n",
    "    prng = Random()\n",
    "    prng.seed(seed)\n",
    "    pf = pf.copy()\n",
    "    values = [portfolio_value(pf)]\n",
    "    \n",
    "    for day in range(days):\n",
    "        pf = simstep(pf, params, prng)\n",
    "        values.append(portfolio_value(pf))\n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now repeat the simulation on eleven of our seeds from the earlier run, collecting history for each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simulated_results.sort()\n",
    "eleven_results = [simulated_results[int((len(simulated_results) - 1) * i / 10)] for i in range(11)]\n",
    "eleven_seeds = sc.parallelize([seed for (_, seed) in eleven_results])\n",
    "walks = eleven_seeds.map(lambda s: simulate_with_history(s, bpf.value, bparams.value, 5))\n",
    "\n",
    "walk_results = walks.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = plt.plot([list(c) for c in zip(*walk_results)])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
